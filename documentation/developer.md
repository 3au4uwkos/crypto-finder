# Руководство разработчика - Crypto Analyzer

## 1. Структура проекта

```
project/
├── config/
│   ├── __init__.py
│   └── settings.py           -- Настройки приложения (размер окна и др.)
├── core/
│   ├── __init__.py
│   ├── data_cleaner.py       -- Предобработка данных для ML
│   └── data_processor.py     -- Обучение модели и предсказания
├── data_collection/
│   ├── __init__.py
│   └── web_parser.py         -- Парсинг данных с cryptorank.io
├── framework/
│   ├── __init__.py
│   └── get_html.py           -- Получение HTML через Playwright
├── ui/
│   ├── pages/
│   │   ├── __init__.py
│   │   ├── dataset_page.py   -- Страница отображения результатов
│   │   └── start_page.py     -- Главная страница
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── change_page.py    -- Навигация между страницами
│   │   └── configure_grid.py -- Настройка сетки tkinter
│   ├── widgets/
│   │   ├── display_dataframe/
│   │   │   ├── __init__.py
│   │   │   └── display_dataframe.py -- Виджет для отображения DataFrame
│   │   ├── __init__.py
│   │   ├── create_button.py  -- Создание кнопок
│   │   └── create_title.py   -- Создание заголовков
│   ├── __init__.py
│   └── main_window.py        -- Главное окно приложения
├── .gitignore
├── main.py                   -- Точка входа в приложение
└── pyproject.toml           -- Зависимости проекта
```

## 2. Описание модулей

### 2.1 Config (Настройки)

**settings.py:**
Содержит конфигурационные параметры приложения:
- `WINDOW_WIDTH`, `WINDOW_HEIGHT`: размеры главного окна
- Другие константы пользовательского интерфейса

### 2.2 Core (ML составляющая)

**data_cleaner.py:**
Реализует предобработку данных для машинного обучения:
- `preprocess(df)`: подготавливает данные для обучения модели
- Использует `StandardScaler` для нормализации числовых признаков
- Применяет `OneHotEncoder` для категориальных признаков
- Разделяет данные на обучающую и тестовую выборки

**data_processor.py:**
Содержит функции для обучения модели и предсказаний:
- `train_model(df)`: обучает модель RandomForestRegressor на исторических данных
- `fill_column(df, model)`: добавляет предсказанные значения ROI в DataFrame
- Использует Pipeline для объединения предобработки и модели

### 2.3 Data Collection (Сбор данных)

**web_parser.py:**
Парсинг данных с сайта cryptorank.io:
- `parse_html_to_dataframe(html, source)`: парсит HTML таблицы в DataFrame
- `get_upcoming(rows, pages)`: собирает данные о предстоящих ICO
- `get_source(pages)`: собирает исторические данные завершенных ICO
- Обрабатывает специальные форматы (K/M для тысяч/миллионов)
- Использует BeautifulSoup4 для парсинга HTML

### 2.4 Framework (Веб-парсинг)

**get_html.py:**
Получение HTML контента веб-страниц:
- `get_html(url, max_retries, delay)`: загружает HTML через Playwright
- Использует headless Chrome для имитации браузера
- Включает механизм повторных попыток при ошибках
- Ожидает загрузки элементов таблиц перед извлечением контента

### 2.5 UI (Пользовательский интерфейс)

**main_window.py:**
Главное окно приложения на tkinter:
- Создает корневое окно с заданными размерами
- Настраивает внешний вид и поведение окна
- Запускает стартовую страницу

**pages/start_page.py:**
Главная страница приложения:
- Отображает заголовок и кнопку генерации
- Обрабатывает запуск процесса анализа данных
- Переключает на страницу результатов

**pages/dataset_page.py:**
Страница отображения результатов:
- Показывает таблицу с проанализированными данными
- Поддерживает ограничение количества отображаемых строк
- Включает кнопку возврата на главную страницу

**widgets/display_dataframe.py:**
Виджет для отображения DataFrame:
- Создает Treeview для табличного отображения данных
- Автоматически настраивает ширину колонок
- Включает вертикальную прокрутку
- Поддерживает центрирование контента

**utils/configure_grid.py:**
Утилиты для настройки сетки tkinter:
- `set_grid()`: настраивает веса строк и столбцов
- `reset_grid()`: сбрасывает настройки сетки
- Упрощает управление layout-ом интерфейса

## 3. Алгоритм работы приложения

1. **Инициализация**: Запуск главного окна и отображение стартовой страницы
2. **Сбор данных**: 
   - Загрузка исторических данных ICO для обучения модели
   - Загрузка данных о предстоящих ICO для анализа
3. **Обучение модели**: 
   - Предобработка исторических данных
   - Обучение RandomForestRegressor на признаках 'fund' и 'raise'
4. **Предсказание**: Применение обученной модели к новым данным
5. **Отображение результатов**: 
   - Сортировка по предсказанному ROI
   - Отображение в табличном виде с возможностью прокрутки

## 4. Технические особенности

### 4.1 Структура данных

**Исторические данные (для обучения):**
- `name`: название проекта
- `roi`: фактическая доходность (целевая переменная)
- `fund`: инвестиционный фонд
- `raise`: сумма привлеченных средств

**Данные для предсказания:**
- `name`: название проекта
- `raise`: планируемая сумма привлечения
- `fund`: инвестиционный фонд
- `date`: дата запуска

### 4.2 Машинное обучение

- **Модель**: RandomForestRegressor (100 деревьев)
- **Признаки**: 'fund' (категориальный), 'raise' (числовой)
- **Целевая переменная**: ROI (return on investment)
- **Предобработка**: StandardScaler + OneHotEncoder
- **Обработка пропусков**: SimpleImputer с стратегией 'most_frequent'

### 4.3 Веб-скрапинг

- **Браузер**: Chromium через Playwright
- **User-Agent**: имитация современного Chrome браузера
- **Таймауты**: 30 сек на загрузку страницы, 10 сек на селекторы
- **Повторные попытки**: до 3 попыток с задержкой 3.5 сек

## 5. Зависимости

Основные библиотеки (согласно pyproject.toml):
- `pandas==2.3.0`: работа с данными
- `numpy==2.3.0`: численные вычисления
- `scikit-learn>=1.7.0`: машинное обучение
- `playwright==1.52.0`: автоматизация браузера
- `beautifulsoup4>=4.13.4`: парсинг HTML
- `tkinter`: пользовательский интерфейс (встроен в Python)

## 6. Расширение функциональности

### 6.1 Добавление новых источников данных
Для добавления новых сайтов:
1. Создайте новую функцию парсинга в `web_parser.py`
2. Адаптируйте структуру данных под новый источник
3. Обновите логику объединения данных

### 6.2 Улучшение модели
- Добавьте новые признаки в `data_cleaner.py`
- Экспериментируйте с другими алгоритмами в `data_processor.py`
- Реализуйте кросс-валидацию для оценки качества модели

### 6.3 Расширение интерфейса
- Добавьте новые страницы в папку `ui/pages/`
- Создайте дополнительные виджеты в `ui/widgets/`
- Реализуйте сохранение результатов в файлы

## 7. Тестирование

Рекомендуемые проверки:
1. **Сбор данных**: проверьте корректность парсинга различных страниц
2. **Обучение модели**: убедитесь в отсутствии ошибок при обучении
3. **Предсказания**: проверьте адекватность предсказанных значений
4. **Интерфейс**: протестируйте навигацию и отображение данных
5. **Обработка ошибок**: проверьте поведение при недоступности сайта